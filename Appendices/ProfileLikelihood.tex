\chapter{Profile likelihood ratio test}\label{app:likelihood}
\lhead{\emph{Appendix: Profile likelihood ratio test}}
The neutrino floor is calculated as a discovery limit and is usually mapped using a profile likelihood ratio test. The procedure comprises a hypothesis test between the null hypothesis $H_0$ (background only) and the alternative hypothesis $H_1$ which includes both background and signal. We can also incorporate systematic uncertainties such as uncertainties on the experimental background, neutrino flux normalisations or astrophysical parameters. First we construct a likelihood function $\mathscr{L}(\sigma_p,\boldsymbol{\Theta})$ which gives the probability of observing a set of data given a certain value of cross section $\sigma_p$, as well some set of other parameters contained in $\boldsymbol{\Theta}$. We then Monte Carlo generate sets of mock WIMP and background data and try to reject the background only hypothesis using the following likelihood ratio,
\begin{equation}
\lambda(0) = \frac{\mathscr{L}(\sigma_p = 0,\hat{\hat{\boldsymbol{\Theta}}})}{\mathscr{L}(\hat{\sigma}_p,\hat{\boldsymbol{\Theta}})},
\end{equation}
where ${\hat{\boldsymbol{\Theta}}}$ and $\hat{\sigma}_p$ denote the values of ${\boldsymbol{\Theta}}$ and $\sigma_p$ that maximise the full likelihood and  
$\hat{\hat{\boldsymbol{\Theta}}}$ denotes the values of ${\boldsymbol{\Theta}}$ that maximise $\mathscr{L}$ under the condition $\sigma_p = 0$, i.e. we are profiling over all of ${\boldsymbol{\Theta}}$ which are considered to be nuisance parameters. As introduced in Ref.~\cite{Cowan:2010js}, the test statistic $q_0$ is then defined as,
\begin{equation}
q_0 = \left\{
\begin{array}{rrll}
\rm & -2\ln\lambda(0)	&	\ \hat{\sigma}_p > 0 \,, \\
\rm & 0  		& 	\ \hat{\sigma}_p < 0. 
\end{array}\right.
\end{equation}
A large value for this statistic implies that the alternative hypothesis gives a better fit to the data,  i.e. that it contains a WIMP signal.
The $p$-value, $p_0$, of a particular experiment is the probability of finding a value of $q_0$ larger than or equal to the observed value, $q_0^{\rm obs}$, if the null (background only) hypothesis is correct:
\begin{equation}
p_0 = \int_{q_0^{\rm obs}}^{\infty} f(q_0|H_0) \, {\rm d} q_0, 
\end{equation}
where $f(q_0|H_0)$ is the probability distribution function of $q_0$ under the background only hypothesis. Following Wilks' theorem, $q_0$ asymptotically follows a half $\chi^2$ distribution with one degree of freedom (see Ref.~\cite{Cowan:2010js} for a more detailed discussion) and therefore the significance $Z$ in units of standard deviation is simply given by $Z = \sqrt{q^{\rm obs}_0}$. The discovery limit for a particular input WIMP mass can then be found by finding the minimum cross section for which 90\% of the simulated experiments have $Z \geq 3$. For calculations of the neutrino floor we typically build a distribution for $Z$ based on 5000 Monte Carlo experiments for each input mass and cross section. Note that mapping discovery limits can be made much more efficient by performing the test first at a fixed mass so that the various factors appearing the likelihood can be rescaled by a constant factor for varying $\sigma_p$. Then once the discovery limit at fixed mass has been found, the test can be repeated over a range of input masses to map the discovery limit.

This analysis methodology was first introduced by the XENON10 collaboration~\cite{Aprile:2011hx} and many experiments are now using similar likelihood approaches e.g. LUX~\cite{Akerib:2013tjd}, CDMS-II~\cite{Billard:2013gfa,Agnese:2014xye}, and CoGeNT~\cite{Aalseth:2014jpa}. This has become possible thanks to the construction of accurate background models derived from reliable simulations, as well as data-driven analysis techniques based on calibration data. The advantage of using likelihood analyses is that they can not only determine whether or not a dark matter interpretation to the data is preferred and a WIMP signal detected, but they can also measure or constrain the WIMP parameters themselves.